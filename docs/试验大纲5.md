# 实验 5：Hankel 谱学习 vs 样本数量与长度

> **目标：** 数值验证第 9–11 章关于
>
> - Hankel 集中性 $\|\widehat H - H\|_2$ vs 样本量 $N$；
> - 端到端误差 $\sup_{|x|=L}|\widehat p_N(x)-p(x)|$ 的样本复杂度和长度依赖；
> - contractive 投影将几何放大 $F(L)\approx L\kappa_B^{L-1}$ 降为近线性 $F(L)\approx L$。

核心理论式子是（忽略 $O(1/N)$ 项）：
$$
\|\widehat H - H\|_2 = O\!\Big(\sqrt{\tfrac{\mu}{N}}\Big),\qquad  
\sup_{|x|=L}|\widehat p_N(x)-p(x)|
\ \lesssim\
\frac{F(L)}{\gamma}\sqrt{\tfrac{\mu}{N}}.
$$

------

## 1. 理论映射与要验证的结论

对应正文定理（主要是 9.1 和 11.5）：

- $H = H_p(P,S)$：长度为 L、mid-cut $t_*=\lfloor L/2\rfloor$ 的概率 Hankel；

- $\gamma = \sigma_r(H)$：H 的最小非零奇异值（有效秩 r）；

- $\mu$：相干度（coherence）
  $$
  \mu := \max\Big\{\max_{u\in P}\sum_v H(u,v),\ \max_{v\in S}\sum_u H(u,v)\Big\};
  $$

- $F(L)$：长度放大因子：

  - 一般谱范数情形：$F(L) \asymp L\kappa_B^{L-1}$，$\kappa_B = \max_\sigma\|B_\sigma\|_2$；
  - 可收缩锥：$F(L)\asymp L$。

**本实验要验证的三个主结论：**

1. **(E5.1) Hankel 集中：**
    $\Delta_N := \|\widehat H - H\|_2$ 随 $N$ 近似 $N^{-1/2}$ 缩放，且归一化量 $\Delta_N\sqrt{N/\mu}$ 约为常数（9.1）。

2. **(E5.2) 端到端误差 vs N（固定 L，稳定 regime）：**
    在若干 **γ 适中、条件数良好** 的 ground truth 上，
   $$
   E_\infty(N) := \sup_{|x|=L}|\widehat p_N(x)-p(x)| \approx C\frac{F(L)}{\gamma}\sqrt{\frac{\mu}{N}},
   $$
   用归一化误差
    $\widetilde E_N := E_\infty(N) \cdot \gamma / F(L)$ 检查其是否与 $\sqrt{\mu/N}$ 平行。

3. **(E5.3) 端到端误差 vs L（固定 N）：**

   - raw 谱学习下，误差随 L 明显超线性增长（几何放大）；
   - 对学得的 $\{\widehat B_\sigma\}$ 做 **行次随机投影** 后，误差随 L 的增长接近线性 $O(L)$，与「可收缩锥 + 非扩张范数」理论相符。

------

## 2. ground truth 模型族与总体设定

### 2.1 字母表与长度

- 字母表：统一采用 $\Sigma = \{0,1\}$（d=2），便于完全枚举字符串。
- 长度集合：拆到两个维度使用：
  - **样本依赖（5a/5b）**：$L \in \{8,10,12\}$（适合枚举，状态空间 $\le 2^{12}=4096$）；
  - **长度依赖（5c）**：$L \in \{5,8,11,14,17\}$，L=17 时可按算力选择枚举或随机抽子集评估 sup。

### 2.2 模型族（与现有脚本对应）exp5_sample_complexity

1. **Model A：低秩 / 低纠缠 MPS（“low”）**
   - 键维 $D_{\text{low}}=2$；
   - 采用 left-canonical 的随机 MPS (`random_left_canonical_mps(length, bond_low, d)`，bond_low=2)；
   - 典型情况：Hankel 有较低的有效秩 r、$\gamma$ 不太小，条件数相对好。
2. **Model B：高熵 MPS（“high”）**
   - 键维 $D_{\text{high}}=4$ 或 5（可以设 bond_high=4）；
   - 同样用 left-canonical MPS 生成；
   - Hankel 有更高有效秩，谱尾可能很小，适合作为“难学/劣条件”对照。
3. **Model C：contractive WFA（推荐用于长度依赖）**
   - 使用 `wfa_ground_truth(d, wfa_dim)` 生成：非负行随机矩阵；
   - $\{B_\sigma\}$ 本身就在非负行次随机锥内；
   - 作为理论中“可收缩锥 + $F(L)=O(L)$”的干净示例。

> 现有脚本中以上三类模型都已经实现，可以直接重用。exp5_sample_complexity

### 2.3 ground truth 固定策略（关键强化点）

为避免之前“每个样本量 N 都重采一套 ground truth”导致的 scaling 混淆，现在 **显式固定 ground truth**：

- 对每个 `(model, L)`：
  - 先采 **K_seed 个**随机 ground truth（例如 K_seed = 20）；
  - 对每个 seed 计算真 Hankel 的 SVD，得到奇异值 $\{s_k\}$、rank_true、$\gamma$ 和 $\mu, \kappa_B$；
  - 将这些信息记录到一个 “seed 表” 中。
- 后续所有 N-sweep 和 L-sweep 实验，**只在同一批 seed 上重复改变 N 或 L**，不再改变 ground truth。这样才能看清“对同一个分布，误差如何随 N/L 缩放”。

------

## 3. Hankel 构造与统一谱学习 pipeline

这部分延续你现有脚本的实现，只在大纲里规范步骤，方便对照。

### 3.1 mid-cut Hankel 与相干度

对每个 ground truth seed 和长度 L：

1. **前缀/后缀集合：**
   - mid-cut $t_*=\lfloor L/2\rfloor$；
   - 生成所有长为 $t_*$ 的字串；若数量 > max_prefixes（如 512），随机抽样到 cap；
   - 后缀同理；
   - 强制包含全 0 的前缀、全 0 的后缀（后续定义边界向量用）。
2. **真 Hankel：**
   - 枚举所有 $x\in\Sigma^L$，用 MPS 或 WFA 生成真概率 $p(x)$；
   - 对每个 $(u,v)\in P\times S$，设 $H(u,v)=p(uv)$（不存在的词概率=0）。
3. **SVD 与有效秩：**
   - 对 $H$ 做 SVD，奇异值 $\{s_k\}$ 降序；
   - 有效秩 $r_{\text{true}} = \#\{k : s_k \ge \texttt{tol\_true} \cdot s_1\}$，推荐 $\texttt{tol\_true}\in[10^{-6},10^{-4}]$；
   - $\gamma := s_{r_{\text{true}}}$。
4. **相干度与 $\kappa_B$：**
   - coherence $\mu$ 按行和/列和最大值定义；
   - 对 MPS 模型，用 `kappa_from_mps` 估计 $\kappa_B = \max_{t,\sigma}\|A_t(\sigma)\|_2^2$；
   - 对 WFA 模型，$\kappa_B = \max_\sigma\|B_\sigma\|_2$。

### 3.2 经验 Hankel $\widehat H$ 和 Δ_N

对每个 `(seed, N)`：

1. 从真分布 $p$ 中抽 N 个长度为 L 的样本（one-shot，独立同分布）；

2. 保留前缀/后缀都落在 $P,S$ 中的样本，有效样本数记作 $N_{\text{eff}}$；

3. 用频率估计 $\widehat H(u,v)$，构成经验 Hankel；

4. Hankel 误差
   $$
   \Delta_N = \|\widehat H - H\|_2,
   $$
   用 `spectral_norm` 对差矩阵做 power iteration 近似谱范数。

### 3.3 SVD 白化与 $\widehat B_\sigma$（修正成“full-width + mask”）

**rank 选择策略：**

- 主实验（验证理论）中：
  - 设置 `rank_used = r_true`（从真 Hankel 的 SVD 得到）；
- 额外实验（看 rank mis-spec 影响）：
  - 可以再跑 `rank_used ∈ {r_true−2, r_true−1, r_true+1}` 一类配置，单独分析。

**白化与 B_sigma：**

1. 对经验 Hankel $\widehat H$ 做 truncated SVD，rank = rank_used，得到 $U_r,\Sigma_r,V_r$；

2. 白化矩阵：
   $$
   W_L = \Sigma_r^{-1/2} U_r^\top,\quad W_R = V_r\Sigma_r^{-1/2};
   $$

3. 对每个符号 $\sigma$：

   - 构造列 mask：`mask[sigma][j] = (suffixes[j] 以 σ 为首字母)`；

   - 定义 full-width masked Hankel：对 mask 为 False 的列置 0，得到 $\widehat H_\sigma^{\text{full}}$；

   - 定义转移矩阵：
     $$
     \widehat B_\sigma = W_L \widehat H_\sigma^{\text{full}} W_R.
     $$

   - 不再重排列索引，只是 0 填充，避免“列错位”问题（你现在的脚本已按这个逻辑实现）。exp5_sample_complexity

4. 记录经验最小奇异值 $\widehat \gamma = \min \text{diag}(\Sigma_r)$。

### 3.4 边界向量和 learned 分布

1. 选择全 0 前缀 $u_0$、全 0 后缀 $v_0$，找出其索引 $i_0,j_0$；

2. 初始向量：$\widehat i^\top = \widehat H(i_0,:)\,W_R$；

3. 终止向量：$\widehat f = W_L\,\widehat H(:,j_0)$；

4. 对任意词 $x$：
   $$
   \widehat p_N(x) = \widehat i^\top \widehat B_{x_1}\cdots \widehat B_{x_L}\widehat f;
   $$
   用 `apply_sequence` 实现。exp5_sample_complexity

5. 将所有 $\widehat p_N(x)$ 截断为非负并归一化，得到最终的 learned 分布。

**误差指标：**

- 点态 max 误差：$E_\infty = \max_x |\widehat p_N(x)-p(x)|$；
- TV 误差：$E_{\text{TV}} = \frac12\sum_x |\widehat p_N(x)-p(x)|$。

------

## 4. 子实验 5a：Hankel 集中 vs 样本数 N

### 4.1 配置与筛选

- 选定长度 $L\in\{8,10,12\}$，选 Model A/B；
- 对每个 `(model, L)` 的 20 个 seed，预先计算 $(r_{\text{true}},\gamma,\mu,\kappa_B)$；
- 全部 seed 都参与 Hankel 集中实验（这一部分不需要筛 γ）。

**样本数网格：**
$$
N \in \{500, 1000, 2000, 5000, 10^4, 2\times 10^4\}
$$
每个 `(model, L, seed, N)` 重复 trial 次（例如 trial = 10），以平均掉采样随机性。

### 4.2 需要记录的量

对每个 run 记录：

- `delta_hankel = \Delta_N`；
- `mu`、`rank_true`、`gamma`（从 seed 表中读）；
- `effective_samples = N_eff`（经验 Hankel 中实际落到 P×S 的样本数）。

### 4.3 预期分析与图形

1. **log–log 图 $\Delta_N$ vs N：**
   - 对每个 `(model,L)`，取所有 seed 的中位数 $\operatorname{med} \Delta_N$；
   - 作图：x 轴 $\log N$，y 轴 $\log \Delta_N$，拟合斜率，预期 ≈ −1/2。
2. **归一化 $\Delta_N\sqrt{N/\mu}$：**
   - 对每个 run，计算 $Q_N = \Delta_N\sqrt{N/\mu}$；
   - 对每个 `(model,L,N)` 做箱线图或误差条，预期 Q_N 随 N 基本恒定，验证 $\Delta_N \sim \sqrt{\mu/N}$。

------

## 5. 子实验 5b：端到端误差 vs N（固定 L，稳定 regime）

这一部分只在 **条件数适中** 的 ground truth 上做，以验证 11.5 中的样本复杂度：
$$
E_\infty(N) \approx C\frac{F(L)}{\gamma}\sqrt{\frac{\mu}{N}}.
$$

### 5.1 稳定 seed 的选择标准

在 5a 里我们已经计算了每个 seed 的 $\gamma, \mu, \kappa_B$。现在：

- 设定阈值：

  - $\gamma_{\text{min}} \approx 10^{-3}$ 或 $10^{-4}$（结合实际数值分布调整）；
  - 以及一个上限 $\gamma_{\text{max}}$（比如 0.1），避免极端简单的情况；

- 只选择满足
  $$
  \gamma_{\text{min}}\le \gamma \le \gamma_{\text{max}}
  $$
  且 rank_true 不太大的 seeds（例如 r_true ≤ 8），作为“稳定 regime”的 ground truth。

对每个 `(model,L)`，尽量挑出 3–5 个这样的 seed 用于 5b。

### 5.2 配置与记录

在 5a 用的样本数网格上（同样的 N 集合），对这些稳定 seed：

- 重新按照统一 pipeline 学习 $\widehat p_N$（rank_used=r_true）；
- 记录：
  - `max_error_raw = E_\infty(N)`；
  - `tv_error_raw`；
  - `scaled_error_raw = E_\infty(N) * gamma / F_raw(L)`，其中 $F_{\text{raw}}(L) = L\kappa_B^{L-1}$；
  - `delta_hankel`（方便计算 $\Delta_N/\gamma$）；

**还需要一个关键指标：**

- `delta_over_gamma = delta_hankel / gamma`：用于判断是否处在 $\Delta_N/\gamma \ll 1$ 区域。

### 5.3 分析与图形

1. **挑选真正稳定的 N：**
   - 对每个 N，统计 $\Delta_N/\gamma$ 的中位数；
   - 选出那些 $\Delta_N/\gamma\le 0.2$ 的 N 作为“真正满足理论假设”的样本数段，对这些点重点分析 scaling。
2. **log–log 图 E∞ vs N：**
   - 对稳定 seed，在“有效 N” 区间绘制 $\log E_\infty(N)$ vs $\log N$；
   - 拟合斜率，预期接近 −1/2。
3. **归一化误差 vs N：**
   - 绘制 $\widetilde E_N = E_\infty(N)\cdot\gamma/F_{\text{raw}}(L)$，对比 $\sqrt{\mu/N}$ 曲线；
   - 观察两者在 log–log 上是否几乎平行。
4. **病态 regime 的对照：**
   - 对同一 `(model,L)` 中 $\gamma$ 很小的 seed，画一两条曲线展示：即便 N 增大，E∞ 也几乎不动，说明“小 γ + 大条件数”如何破坏学习，呼应理论中的 warning。

------

## 6. 子实验 5c：端到端误差 vs 长度 L（固定 N）

目标：检验 length factor $F(L)$ 的几何 vs 线性差异，重点展示 contractive 投影后误差随 L 的“近线性”增长。

### 6.1 配置

- 选 Model C（contractive WFA）作为主角；可以加一个 Model B 作为“几何放大”的对照。exp5_sample_complexity
- 固定一个“中等偏大”的样本数 $N_0$，如 $N_0=10^4$；
- 长度集合：$L \in \{5,8,11,14,17\}$；
- 对每个 `(model, L)`，采多种 seed（如 10 个），保证覆盖不同的 $\kappa_B$。

### 6.2 raw vs projected pipeline

对每个 `(seed,L)`：

1. 用 $N_0$ 个样本估计 Hankel、构建 $\widehat B_\sigma$、$\widehat i,\widehat f$（rank_used=r_true）；

2. raw 模型：直接用 $\{\widehat B_\sigma\}$ 计算 $\widehat p_N^{\text{raw}}$，得到
   $$
   E_\infty^{\text{raw}}(L),\quad E_{\text{TV}}^{\text{raw}}(L).
   $$

3. contractive 投影：对 $\{\widehat B_\sigma\}$ 应用 `project_row_substochastic`：

   - 按行取绝对值；
   - 对每一行跨所有 σ、所有 j 的总和 >1 的情况统一缩放到 1；
   - 得到 $\{\widehat B_\sigma^{\text{proj}}\}$；

4. projected 模型：用同样的 $\widehat i,\widehat f$ 计算 $\widehat p_N^{\text{proj}}$，得到
   $$
   E_\infty^{\text{proj}}(L),\quad E_{\text{TV}}^{\text{proj}}(L).
   $$

### 6.3 统计与图形

对每个 L，取多个 seed 的平均/中位数：

- $\overline E_{\text{raw}}(L),\ \overline E_{\text{proj}}(L)$；
- 绘制：
  - 图 1：L 为横轴，$\overline E_{\text{raw}}(L)$ 与 $\overline E_{\text{proj}}(L)$ 同图（log y 轴）；
  - 可加参考线 $cL$ 或 $c\kappa_B^{L}$ 作为视觉对照。

**预期：**

- 对 WFA ground truth：
  - raw 版在 L 上的增长明显超线性（特别是如果估计噪声叠加使 $\kappa_B >1$）；
  - proj 版的增长接近线性，小 L 区间与 $cL$ 几乎平行；
- 对高熵 MPS ground truth：
  - raw 曲线可接近几何增长；
  - proj 曲线增长速度显著下降，验证“投影到可收缩锥削弱了几何放大”。

------

## 7. 子实验 5d（可选）：rank mis-spec 对端到端误差的影响

为呼应理论中“rank / 有效秩匹配”的要求，可以设计一组 rank mis-spec 实验：

- 选几个条件数不错的 seed（γ 适中，r_true 中等）；
- 对固定 `(model,L)`，同时跑：
  - rank_used=r_true；
  - rank_used=r_true−2；
  - rank_used=r_true+1（over-param）；
- 分别测量 E∞ vs N，比较：
  - rank_used<r_true 时，端到端误差的收敛速度是否明显变差、甚至出现饱和；
  - rank_used>r_true 时，是否在小 N 区域引入过拟合噪声。

这个部分可以写成一小节讨论/图 1–2 张，说明 spectral 方法对 rank 设定的敏感性。

------

## 8. 日志字段与脚本对齐建议

你现有的脚本 `exp5_sample_complexity.py` 已经在输出 CSV 中记录了大部分我们需要的字段：`delta_hankel, sigma_min_true, mu, kappa_B, F_raw, max_error_raw, max_error_proj, scaled_error_*` 等。exp5_sample_complexity

为了完全匹配新大纲，建议确保/补充：

1. **seed 级别的固定：**
   - 在 CSV 中显式记录 `seed_id` 或 `ground_truth_id`，保证不同 N/L 实验可以按 seed 对齐；
   - 不同 N 的 run 对应同一 ground truth，而不是每次重采。
2. **Δ/γ 分析用字段：**
   - 可以直接输出一列 `delta_over_gamma = delta_hankel / sigma_min_true`（对 γ=0 设为 NaN），方便筛选稳定 regime；
3. **rank 信息：**
   - 保留 `rank_true, rank_used` 字段，区分 rank 正确和 mis-spec 的 run；
4. **模型标识：**
   - `model ∈ {low, high, contractive}` 已经有；
   - 若未来扩展其他模型，可加 `model_family` 或 `model_config` 说明参数。

------

## 9. 论文写法示意（可直接移植）

最后给一个可以直接放进 paper 的 summary 段落，与你新的大纲严格对应：试验大纲5

> **Hankel concentration vs. sample size.**
>  We instantiate the mid-cut Hankel matrix $H_p(P,S)$ using prefix/suffix sets $P,S$ of size at most 512 and evaluate the spectral deviation $\Delta_N = \|\widehat H - H\|_2$ from $N$ i.i.d. samples. Across several ground-truth MPS and WFA models and lengths $L\in\{8,10,12\}$, $\Delta_N$ decays at rate $N^{-1/2}$ on log–log scales, while the rescaled quantity $\Delta_N\sqrt{N/\mu}$ remains nearly constant, in line with the matrix Bernstein bounds in Section 9.
>
> **End-to-end spectral learning vs. sample size.**
>  For well-conditioned Hankel matrices (seeds with moderate smallest non-zero singular value $\gamma$), and after setting the spectral rank to the true Hankel rank, the pointwise reconstruction error $E_\infty(N)=\sup_{|x|=L}|\widehat p_N(x)-p(x)|$ exhibits an approximate $N^{-1/2}$ decay. Once normalised by the smallest singular value and the length factor, $\widetilde E_N = E_\infty(N)\cdot\gamma/F(L)$, the curves closely track $\sqrt{\mu/N}$, providing empirical support for Theorem 11.5.
>
> **Length dependence and contractive projection.**
>  For a contractive WFA ground truth and several MPS-based models, we fix a moderate sample size and vary the sequence length $L\in\{5,8,11,14,17\}$. The raw spectral learner displays pronounced super-linear growth of $E_\infty(L)$, consistent with a geometric length factor $F(L)\approx L\kappa_B^{L-1}$. After projecting the learned operators onto a nonnegative row-substochastic cone, the error growth becomes nearly linear in $L$, especially in the WFA case, corroborating our contractive-norm analysis where $F(L)=O(L)$.