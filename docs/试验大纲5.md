## 实验 5：Hankel 谱学习 vs 样本数量

（验证第 9–11 章端到端误差界）

### 1. 理论目标回顾

主公式（11.5）：
$$
\sup_{|x|=L}|\widehat p(x)-p(x)|
\ \lesssim\
\frac{F(L)}{\gamma}\left(
\sqrt{\frac{\mu}{N}} + O\left(\frac{1}{N}\right)
\right),
$$
其中：

- $\gamma = \sigma_r(H)$：目标 Hankel $H$ 的最小非零奇异值；
- $\mu$：相干度（第 9 章），控制方差；
- $F(L)$：长度放大因子：
  - 一般情形：$F(L) \asymp L\,\kappa_B^{L-1}$（几何放大）；
  - 可收缩锥情形：$F(L)\asymp L$。

**实验目标：**

1. **5a：误差 vs N（固定 L）**
   - 检查 $\sup_x|\widehat p_N(x)-p(x)|$ 是否随 $N$ 近似 $N^{-1/2}$ 缩放；
   - 把误差乘上 $\gamma/F(L)$，看是否和 $\sqrt{\mu/N}$ 的曲线对齐（只差常数）。
2. **5b：误差 vs L（固定 N）**
   - 在原始谱学习下，误差随 L 明显超线性（几何放大）；
   - 在对学得的 $\{\widehat B_\sigma\}$ 做可收缩锥投影后，误差随 L 的增长接近线性，符合 “$F(L)$ 从 $L\kappa^{L-1}$ 缩到 $L$” 的结论。

------

## 2. 统一的 ground truth 模型与 Hankel pipeline

### 2.1 ground truth 模型选择

建议使用 2–3 个代表性 MPS/QCBM 模型，覆盖不同结构：

1. **Model A：低秩 / 低纠缠 MPS**
   - 小键维 $D=2$ 或 3；
   - 由浅层随机电路或直接随机 MPS 生成；
   - Hankel rank r 比较小、$\gamma$ 不太小 ⇒ 条件数较好。
2. **Model B：高纠缠 / 更高秩 MPS**
   - 较大键维 $D=4$ 或 5；
   - 深一点的随机电路生成；
   - Hankel 秩 r 大一点，$\gamma$ 较小 ⇒ 更难学。
3. **（选做）Model C：接近非负/次随机的 WFA 型模型**
   - 可以直接指定一组非负转移矩阵 $B_\sigma\in\mathcal K_{\rm row}$，再反推 MPS，或者干脆把 WFA 当 ground truth 源；
   - 让 “可收缩锥投影” 更贴近真值，便于观察理论中的线性 L 依赖。

> 为了实验一致性，**Hankel 学习 pipeline 对所有模型完全一样**，只是 MPS 参数不同。

### 2.2 Hankel 构造设定

对每个模型：

- 字母表：用 $\Sigma = \{0,1\}$（或 $|\Sigma|=4$ 做一个对比，但二元足够）。
- 长度 L 的范围（用于 5a, 5b 不同）：
  - 5a：固定一个 $L^\star\in\{8,10,12\}$；
  - 5b：L 取一组值，如 $\{5,8,11,14\}$（长一点的时候只算误差在随机子集上）。

**统一 Hankel 视角：**

- 再次采用**固定切分点**：
  $$
  t_\* = \lfloor L/2\rfloor
  $$

- 前缀集合 $P\subseteq\Sigma^{t_\*}$：

  - 若 $|\Sigma|^{t_\*}$ 不太大（如 ≤256），取全部前缀；
  - 否则随机选取固定数目 $m_P$ 个前缀（如 256 个）。

- 后缀集合 $S\subseteq\Sigma^{L-t_\*}$：同理，大小为 $m_S$（可取和 $m_P$ 相同）。

最终 Hankel 维度约为 $m_P\times m_S$，保证 SVD 十分轻量。

**真 Hankel：**

- 对所有 $u\in P, v\in S$，构造 $x=uv$，直接用 MPS 计算
  $$
  p(x)=|\psi(x)|^2
  $$
  作为真值；

- 记真 Hankel：
  $$
  H(u,v)=p(uv),
  $$
  对应模型的 rank 为 $r = \mathrm{rank}_\tau(H)$，用数值阈值（如 $\tau=10^{-8}$）定义。

- 奇异值：

  - 记 $\gamma=\sigma_r(H)$ 为最小非零奇异值；

  - 记相干度 $\mu$ 为：
    $$
    \mu := \max\Big\{\max_{u\in P}\sum_{v}|H(u,v)|,\,\max_{v\in S}\sum_{u}|H(u,v)|\Big\}.
    $$

这些量在整个实验中固定，用于缩放误差。

------

## 3. 光谱学习 pipeline（用于所有 5a/5b）

给定：模型、L、P,S、真 Hankel $H$，样本数 N。

### 3.1 采样 & 经验 Hankel

1. 从模型出发，采 N 个长度为 L 的样本 $x^{(1)},\dots,x^{(N)}$（one-shot 与论文一致）。

2. 每个样本按 $t_\*$ 切分：$x^{(i)}=u^{(i)}v^{(i)}$，其中 $u^{(i)}\in\Sigma^{t_\*}, v^{(i)}\in\Sigma^{L-t_\*}$。

3. 若 $u^{(i)}\notin P$ 或 $v^{(i)}\notin S$，直接忽略该样本；

4. 统计频数：
   $$
   \widehat H(u,v)
   = \frac{1}{N}\sum_{i=1}^N \mathbf 1\big\{u^{(i)}=u,\ v^{(i)}=v\big\}.
   $$
   （也可以用“有效样本数” $N_{\text{eff}}$ 代替 N，作为归一化因子，但只要两边一致，缩放没问题。）

5. 为 5a，用真值 $H$ 直接计算 Hankel 估计误差：
   $$
   \Delta_N := \|\widehat H - H\|_2.
   $$

### 3.2 SVD & 白化（rank‑r）

1. 对 $\widehat H$ 做 SVD：
   $$
   \widehat H = U\Sigma V^\top;
   $$

2. 取前 r 个奇异向量：
   $$
   U_r\in\mathbb R^{m_P\times r},\quad
   \Sigma_r\in\mathbb R^{r\times r},\quad
   V_r\in\mathbb R^{m_S\times r}.
   $$

3. 白化矩阵：
   $$
   W_L := \Sigma_r^{-1/2} U_r^\top,\qquad
   W_R := V_r \Sigma_r^{-1/2},
   $$
   在秩‑r 子空间上有 $W_L\widehat H W_R\approx I_r$。

### 3.3 $\widehat H_\sigma$ 与 $\widehat B_\sigma$

**构造 $\widehat H_\sigma$：**

- 对每个 $\sigma\in\Sigma$，对后缀集合 S 做划分：
  - 定义 $S_\sigma\subseteq S$ 包含所有以 $\sigma$ 开头的后缀：$v=\sigma v'$。
  - 对 $v\in S_\sigma$，把列对应的条目抽出来，得到子矩阵 $\widehat H(:,S_\sigma)$，记作 $\widehat H_\sigma$。

**定义转移矩阵：**
$$
\widehat B_\sigma := W_L\widehat H_\sigma W_R.
$$
（这是第 6 章白化构造的经验版。）

### 3.4 边界向量 $\widehat i,\widehat f$

有多种方式，这里给一个简单可实现、且在两个设定里都一致的方案：

1. 选择一个“空前缀” $u_0$ 与“空后缀” $v_0$ 的代理，比如令：

   - $u_0$ 取全 0 的前缀（长度 $t_\*$）；
   - $v_0$ 取全 0 的后缀（长度 $L-t_\*$）。

2. 估计初始向量：
   $$
   \widehat i^\top := W_L\widehat H(u_0,:),
   $$
   即 “从 u₀ 所在行的 Hankel 向量，投到白化坐标”。

3. 估计终止向量：
   $$
   \widehat f := \widehat H(:,v_0)^\top W_R,
   $$
   即 “从 v₀ 所在列的 Hankel 向量，投到白化坐标”。

> 这一构造不一定是理论上精确的最优选择，但对比实验只要两种设定采用相同规则即可。

### 3.5 learned 分布 $\widehat p_N(x)$

对任意 $x=(x_1,\dots,x_L)$：
$$
\widehat p_N(x) = \widehat i^\top \,\widehat B_{x_1}\cdots\widehat B_{x_L}\,\widehat f.
$$
作为 learned WFA 的输出概率（一般需要截断负值/归一化，见后面误差度量）。

------

## 4. 实验 5a：误差 vs 样本数 N（固定 L）

### 4.1 参数

- 选择一个代表性的模型（比如 Model A 和 Model B 各来一组）。

- 固定一个 $L^\star\in\{8,10,12\}$；

- 预先计算：真 Hankel $H$、rank r、$\gamma=\sigma_r(H)$、相干度 $\mu$。

- 样本数网格：
  $$
  N \in \{500,1000,2000,5000,10^4,2\times 10^4,\dots\}
  $$
  覆盖一两数量级。

- 对每个 N，重复 K 次（例如 $K=20$）以估计平均/中位数误差。

### 4.2 误差指标

对每个 run（固定模型、L, N）：

1. **Hankel 估计误差：**
   $$
   \Delta_N = \|\widehat H-H\|_2.
   $$

2. **点态误差（严格版本）：**

   - 若 $|\Sigma|^{L^\star}$ 不太大（如 $2^{12}=4096$），枚举全部 x：
     $$
     E_\infty := \max_x |\widehat p_N(x)-p(x)|.
     $$

3. **点态误差（近似版本）：**
    若 L 再大一点，可从真分布 p 中再抽一批验证集 $x^{(j)}$，定义
   $$
   E_\infty^{\text{(val)}} := \max_j |\widehat p_N(x^{(j)})-p(x^{(j)})|.
   $$

4. 为避免数值上的负概率 / 总和 ≠1，可以对 $\{\widehat p_N(x)\}$ 做：

   - 截断：$\widehat p_N(x)\leftarrow\max(\widehat p_N(x),0)$；
   - 归一化：$\widehat p_N(x)\leftarrow\widehat p_N(x)/\sum_x\widehat p_N(x)$。

对每个 N，在 K 个 run 上取：

- $\mathrm{med}\,\Delta_N,\ \mathrm{med}\,E_\infty$；
- 或平均值和标准差。

### 4.3 验证 scaling：error vs N

画两类图：

1. **log–log 图：$E_\infty$ vs N**

   - x 轴 $\log N$，y 轴 $\log E_\infty$；
   - 对每个模型画一条曲线（中位数/均值）。
   - 预期斜率 $\approx -1/2$，符合 $\sqrt{\mu/N}$ scaling。

2. **归一化后的 scaling（对应公式）**

   理论式子大致是：
   $$
   E_\infty \times \frac{\gamma}{F(L^\star)}
   \approx C\sqrt{\frac{\mu}{N}}.
   $$
   实验中可以做：

   - 计算归一化误差：
     $$
     \tilde E_N := E_\infty \cdot \frac{\gamma}{F(L^\star)},
     $$
     其中 $F(L^\star)$ 可以按理论选择：

     - 一般情形：估计 $\kappa_B = \max_\sigma\|B_\sigma\|_2$，令 $F(L^\star)= L^\star\,\kappa_B^{L^\star-1}$；
     - 或简单地取 $F(L^\star)=L^\star$（只比较 N scaling，忽略长度因子）。

   - 同时画参考曲线：
     $$
     C\sqrt{\frac{\mu}{N}}
     $$
     （只差一个拟合常数 C）。

   - 若实验点在 log–log 图上与 $\sqrt{\mu/N}$ 方向平行，说明理论 scaling 是合理的。

------

## 5. 实验 5b：误差 vs 长度 L（固定 N）

这一部分要对比：**原始谱学习** vs **可收缩锥投影后** 的 L 依赖。

### 5.1 参数

- 选择一个模型（最好是 Model B 或 C，条件数不算太好，更容易看到长度效应）。

- 固定样本数 N（例如 $N=5\times 10^3$ 或 $10^4$）。

- 取一组长度：
  $$
  L \in \{5,8,11,14,17\}
  $$
  （根据算力可以调整；L 太大时用随机子集近似 sup）。

- 对每个 L，重复学习实验 K 次（比如 K=10）。

### 5.2 原始谱学习误差 vs L

对每个 L：

1. 用统一 pipeline（3.1–3.5）构造 $\widehat p_N$；
2. 计算点态误差 $E_\infty(L)$ 或其近似；
3. 在 K 次重复中取中位数/均值 $\overline E_{\text{raw}}(L)$。

画图：

- x 轴：L；
- y 轴：$\overline E_{\text{raw}}(L)$（可用 log 轴）。

预期：

- 若 $\kappa_B>1$，误差随 L 超线性增长，在半 log 图中近似一条斜率 >0 的直线（对应 $F(L)\sim\kappa^{L}$）。

### 5.3 可收缩锥投影后的误差 vs L

在同一组实验上，再加一步：**对 $\{\widehat B_\sigma\}$ 做可收缩锥投影**，用投影后的 WFA 来定义 $\widehat p_N^{\text{proj}}$。

#### 5.3.1 投影到行次随机锥 $\mathcal K_{\rm row}$（复习 8.4 & 实验 3）

1. 对每个 $\sigma$，考虑 $\widehat B_\sigma\in\mathbb C^{r\times r}$；

2. 取元素绝对值：
   $$
   C_\sigma(i,j) := |\widehat B_\sigma(i,j)|\ge0;
   $$

3. 对每一行 i，统计：
   $$
   r_i := \sum_\sigma\sum_j C_\sigma(i,j);
   $$

4. 缩放因子：
   $$
   s_i = \min\{1,\,1/r_i\};
   $$

5. 投影：
   $$
   \widehat B_\sigma^{\text{proj}}(i,j) := s_i\,C_\sigma(i,j).
   $$

如此得到一组非负且行和 ≤1 的转移矩阵 $\{\widehat B_\sigma^{\text{proj}}\}\in\mathcal K_{\rm row}$，在 $\|\cdot\|_\infty$ 范数下**非扩张**。

> 边界向量 $\widehat i,\widehat f$ 保持不变，或者也可截断为非负并归一化。

#### 5.3.2 用投影后的模型定义 $\widehat p_N^{\text{proj}}$

对任意 x：
$$
\widehat p_N^{\text{proj}}(x)
 = \widehat i^\top \widehat B_{x_1}^{\text{proj}}\cdots \widehat B_{x_L}^{\text{proj}}\widehat f,
$$
同样做截断 + 归一化保证是概率分布。

对每个 L：

1. 计算点态误差
   $$
   E_\infty^{\text{proj}}(L)
   := \sup_x |\widehat p_N^{\text{proj}}(x)-p(x)|.
   $$

2. 在 K 次重复中取均值/中位数 $\overline E_{\text{proj}}(L)$。

画图：

- 在同一坐标系下画 $\overline E_{\text{raw}}(L)$ 与 $\overline E_{\text{proj}}(L)$。
- 对比二者的增长趋势。

**预期：**

- 原始谱学习：曲线随 L 快速上升（几何增长）；
- 投影后：曲线的 slope 明显减小，更接近线性增长（或缓慢的多项式增长），符合理论中 “长度放大因子从 $G_L(\kappa)$ 降到 O(L)” 的结论。

------

## 6. 典型论文写法示例

在第 11 章的实验总结段可以用类似描述：

> We perform an end-to-end spectral learning experiment on several ground-truth QCBM/MPS models. For a fixed length $L$, the reconstruction error $\sup_{|x|=L}|\widehat p_N(x)-p(x)|$ decays approximately as $N^{-1/2}$ on a log-log scale, in line with the $\sqrt{\mu/N}$ scaling predicted by our matrix Bernstein analysis. After normalizing by the smallest non-zero singular value $\gamma=\sigma_r(H)$ and the length-dependent factor $F(L)$, the error curves collapse to a near-straight line with slope $-1/2$, providing empirical support for our bound in Theorem 11.5.
>
> For fixed sample size $N$, we then vary the sequence length $L$ and compare the raw spectral learner with a contractive-cone projected variant, where the learned transition operators are projected onto a nonnegative row-substochastic cone. In the raw setting, the error exhibits a pronounced super-linear (often close to exponential) growth in $L$, reflecting the geometric amplification factor $F(L)\approx L\kappa_B^{L-1}$. In contrast, after projection the error growth becomes almost linear in $L$, consistent with our contractive-norm analysis in Theorem 8.1(B) and Theorem 11.4, where the length dependence is reduced to $F(L)=O(L)$.