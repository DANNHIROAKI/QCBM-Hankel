## 实验 4：条件数与“坏例子”（第 7 章）

### 总体目标

1. **验证第 7 章构造的特殊 MPS：**
   - rank-1 对角 MPS（7.2A） ⇒ Hankel 真正秩=1；
   - rank-3 对角 MPS（7.2B） ⇒ Hankel 秩=3，但第三奇异值随 L 指数衰减，导致严重劣条件。
2. **连接到谱学习样本复杂度：**
   - 在这两个模型上跑 Hankel-谱学习（白化 + WFA 重构），考察：
     - 对同一目标误差 ε，rank-3 “坏例子”比 rank-1 好例子需要多少倍样本；
     - 是否与理论 scaling $N \gtrsim \mu/\sigma_{\min}^2$ 的直觉一致。

------

## Part A：构造坏例子 & 测量 Hankel 的 σ₃(L)

### 1. 模型构造：对角 MPS 7.2A / 7.2B

为简洁，统一采用：

- 键维：$D = 2$
- 字母表：$\Sigma=\{0,1\}$

#### 1.1 rank-1 MPS（7.2A）

定义：

- MPS 张量（不随 t 变化，时间齐次）：
  $$
  A(0) = \begin{pmatrix}1 & 0\\ 0 & \eta\end{pmatrix},\quad
  A(1) = \begin{pmatrix}\eta & 0\\ 0 & 1\end{pmatrix},\quad 0<\eta<1.
  $$

- 边界向量：
  $$
  \alpha = \beta = e_1 = (1,0)^\top.
  $$

对任意串 $x\in\{0,1\}^L$，乘积仍是对角矩阵：
$$
\prod_{t=1}^L A(x_t)
 = \begin{pmatrix}
      \prod_t A(x_t)_{11} & 0\\
      0 & \prod_t A(x_t)_{22}
   \end{pmatrix}.
$$
因为 $\alpha^\top[1,0],\ \beta=[1,0]^\top$，只取左上角：

- 若 $x_t=0\Rightarrow A(0)_{11}=1$，
- 若 $x_t=1\Rightarrow A(1)_{11}=\eta$，

所以振幅
$$
\psi(x) = \eta^{\#1(x)},\quad
p(x)=|\psi(x)|^2=\eta^{2\#1(x)}.
$$
这一构造理论上保证 **Hankel 秩=1**。

#### 1.2 rank-3 “近奇异” MPS（7.2B）

在同一 $A(0),A(1)$ 上，仅改变边界：

- 取参数 $0，

- 边界向量：
  $$
  \alpha = \beta = \begin{pmatrix}1\\ c\end{pmatrix}.
  $$

此时振幅是两个“通道”的线性组合（第一、第二对角元轨道）：

- 第一通道：振幅 $\eta^{\#1(x)}$；
- 第二通道：振幅 $\eta^{\#0(x)}$，其中 $\#0(x)=L-\#1(x)$。

因此可写成：
$$
\psi(x)=\eta^{\#1(x)} + c^2\eta^{\#0(x)},
$$
由此得到概率：
$$
p(x) = |\psi(x)|^2 = \big(\eta^{\#1(x)} + c^2\eta^{\#0(x)}\big)^2.
$$
文中分析给出：对应 Hankel 矩阵

- 可写作三个 rank-1 矩阵之和 ⇒ $\mathrm{rank}(H_p)\le 3$；
- 第三个奇异值 $\sigma_3(H_p)$ 的主导项随 L 约为 $O\big((\sqrt2\,\eta)^L\big)$ 量级，指数衰减。

我们在实验中选取一组具体参数（例如 $\eta=0.6,\ c=0.5$），使得：

- $\sqrt2\,\eta < 1$ ⇒ $\sigma_3$ 指数衰减明显；
- $c$ 不太小，确保前两个奇异值仍然很大。

### 2. Hankel 的构造与奇异值计算

#### 2.1 Hankel 的定义（固定切分点）

与前几次实验保持一致，对给定长度 L：

- 切分点 $t_*=\lfloor L/2\rfloor$，
- 前缀集合 $P = \Sigma^{t_*}$，后缀集合 $S = \Sigma^{L-t_*}$。

对任意 $u\in P, v\in S$：

- 拼接成长度 L 串 $x=uv$；

- 对 rank-1 / rank-3 模型分别计算：
  $$
  p^{(L)}(x)\quad\Rightarrow\quad 
  H_p^{(L)}(u,v)=p^{(L)}(uv).
  $$

> 复杂度备注：
>
> - 对 $L\le 20$，$|P|,|S|\le 2^{10}=1024$，Hankel 为 $1024\times 1024$，可直接 SVD。
> - 串数 $2^L\le 10^6$，也可以仅用“按 (u,v) 组合”遍历（无须存整个分布）。

#### 2.2 SVD & 奇异值提取

对每个 L 和每个模型（rank-1 / rank-3）：

1. 对 $H_p^{(L)}$ 做 SVD：
   $$
   H_p^{(L)} = U\Sigma V^\top,\quad
   \Sigma=\mathrm{diag}(\sigma_1,\sigma_2,\dots).
   $$

2. 记录**前三个奇异值**：

   - rank-1 模型预期：$\sigma_1>0,\ \sigma_2\approx\sigma_3\approx 0$；
   - rank-3 模型预期：$\sigma_1,\sigma_2$ 很大，$\sigma_3$ 非零但随 L 迅速变小。

为减少数值噪声，判定“0”时可用相对阈值，例如：

- 若 $\sigma_i < 10^{-10}\,\sigma_1$，视为数值 0。

### 3. 指标与拟合

#### 3.1 rank-1 模型的 sanity check

- 对 rank-1 MPS：
  - 验证 $\mathrm{rank}_\tau(H_p^{(L)})=1$（仅有一个大奇异值，其余<阈值）；
  - 确认 $\sigma_1$ 随 L 的变化是合理的（例如随 L 有某种增长，但对条件数不敏感）。

这主要是用来说明：**这个例子没有“坏”的条件数问题**。

#### 3.2 rank-3 模型：σ₃(L) 的指数衰减

- 对 rank-3 模型，在多个 L 值上记录 $\sigma_1(L),\sigma_2(L),\sigma_3(L)$；
- 画两类图：
  1. $\sigma_1,\sigma_2,\sigma_3$ vs L 的曲线（线性坐标），看整体趋势；
  2. **重点**：$\log\sigma_3(H_p^{(L)})$ vs L 的散点图。

对后一张图做线性拟合：
$$
\log\sigma_3(H_p^{(L)}) \approx a + bL,
$$
期望 $b<0$，且拟合程度良好 ⇒ **验证 σ₃(L) 约按某个底数 <1 的幂律衰减**。

同时可以报告条件数：
$$
\kappa_L := \frac{\sigma_1(H_p^{(L)})}{\sigma_3(H_p^{(L)})}
$$
随 L 指数增长，体现 Hankel 的劣条件。

> 论文里的典型一句话可以是：
>  “在 rank-3 对角 MPS 例子中，我们观察到 $\log\sigma_3(H_p^{(L)})$ 与 L 几乎线性相关，提示 $\sigma_3(H_p^{(L)})$ 随 L 指数衰减，而前两个奇异值保持在常数级别，从而条件数 $\kappa_L$ 指数增长。”

------

## Part B：谱学习样本复杂度 vs 条件数

这一部分要做的是：**在 rank-1 和 rank-3 模型上用同一谱学习算法训练，比较在同一目标误差下所需样本数 N 的差异。**

### 1. 谱学习 pipeline（简化版）

我们用你论文里的 Hankel-白化框架（第 6–11 章），简化为以下步骤：

1. 固定长度 L（比如 L=10 或 12）和切分点 $t_*=\lfloor L/2\rfloor$，固定前缀/后缀集合 P,S（如“全体”或随机子集）。

2. 对目标分布 $p(x)$（rank-1 或 rank-3），存在真实 Hankel：
   $$
   H = H_p^{(L)}(P,S).
   $$
   最小秩 $r=1$ 或 $r=3$，对应真实最小奇异值 $\gamma=\sigma_r(H)$。

3. 给定样本量 N，从 $p(x)$ 中 i.i.d. 抽样 $x^{(1)},\dots,x^{(N)}$，构造经验 Hankel $\widehat H$。

4. 对 $\widehat H$ 做秩-r 截断 SVD：
   $$
   \widehat H \approx U_r\Sigma_r V_r^\top.
   $$

5. 构造白化矩阵：
   $$
   W_L = \Sigma_r^{-1/2} U_r^\top,\quad
   W_R = V_r\,\Sigma_r^{-1/2},
   $$
   则 $W_L\widehat H W_R\approx I_r$。

6. 对于每个符号 $\sigma$，从样本中构建对应的子块 Hankel $\widehat H_\sigma$（例如：对每个样本 $x=u\sigma v$，在对应的 $(u,v)$ 条目累加一次），再做：
   $$
   \widehat B_\sigma := W_L\widehat H_\sigma W_R.
   $$

7. 边界向量 $\hat i,\hat f$ 可按标准谱学习做法从 Hankel 的“空前缀/空后缀行/列”恢复，或用你论文中“恒等校准”那套构造（这里可以直接引用，不必再详细展开）。

最终得到一个 r 维的 WFA 表示 $(\hat i,\{\widehat B_\sigma\},\hat f)$，定义估计分布：
$$
\widehat p(x_1\cdots x_L)=\hat i^\top \widehat B_{x_1}\cdots\widehat B_{x_L}\hat f.
$$

> 实现建议：
>
> - rank-1 模型：设 r=1，白化与反演非常简单；
> - rank-3 模型：设 r=3，显式考察最小奇异值导致的放大效应。

### 2. 经验 Hankel 的构造细节

对给定 N 个样本 $x^{(i)}\in\Sigma^L$：

1. 令 $c_{uv}$ 为前缀/后缀对 (u,v) 的样本计数：

   - 对每个样本 $x^{(i)}$，将其切分为 $x^{(i)}=u^{(i)}v^{(i)}$；
   - $c_{u^{(i)}v^{(i)}}++$。

2. 经验 Hankel：
   $$
   \widehat H(u,v)=\frac{c_{uv}}{N}.
   $$

3. 子块 Hankel $\widehat H_\sigma$：

   - 可以定义为只统计满足“cut 点后一个符号为 σ”的样本；
   - 更一般的实现是：你在构造时根据 Hankel 分块定义（如“按首字母分块”）来索引。

> 关键点：
>
> - 不需要在实验说明中纠结所有 indexing 小细节，只要和你论文中 H, H_σ 的定义一致即可；
> - 对读者说清楚：“这里我们完全按第 X 节的 Hankel 分块规则来构造 $\widehat H_\sigma$”就行。

### 3. 指标：学习误差 vs 样本数 N

对固定模型（rank-1 or rank-3）和固定 L：

1. 选取一系列样本数 $N \in \{10^3, 3\times 10^3, 10^4, 3\times 10^4, 10^5,\dots\}$。

2. 对每个 N，重复 $R$ 次（例如 $R=20$）：

   - 抽 N 个样本；

   - 构造 $\widehat H$，运行谱学习得到 $\widehat p$；

   - 枚举所有 $x\in\Sigma^L$，计算误差指标，例如：

     - 最大点态误差：
       $$
       E_{\infty} = \sup_{x\in\Sigma^L} |\widehat p(x)-p(x)|;
       $$

     - 或 TV 距离：
       $$
       E_{\mathrm{TV}} = \frac12 \sum_{x\in\Sigma^L} |\widehat p(x)-p(x)|.
       $$

3. 对这 R 次实验取平均，得到：
   $$
   \overline E_{\text{rank-1}}(N),\quad
   \overline E_{\text{rank-3}}(N).
   $$

**预期：**

- 两个模型的误差都大致随 $N^{-1/2}$ 下降（矩阵 Bernstein 支配）；
- 但 rank-3 的曲线在同一 N 上明显更高，且达到同一目标误差 ε 时所需的样本数 $N_\varepsilon$ 显著更大。

### 4. 连接 σ_min(H) 与样本复杂度

对这两个模型，我们在 Part A 中已经得到：

- rank-1 模型：秩 r=1，唯一非零奇异值 $\sigma_1^{(1)}(H)$ 适中（不随 L 快速衰减）；
- rank-3 模型：秩 r=3，最小奇异值 $\sigma_3^{(3)}(H)$ 随 L 指数衰减。

理论上（第 7.3 节 & 11.5–11.6）：

- 经验 Hankel 误差 $\Delta = \|\widehat H-H\|_2$ 约为 $\tilde O(\sqrt{\mu/N})$；

- 白化/反演把误差放大 ~ $1/\sigma_{\min}(H)$；

- 点态误差大致 $\varepsilon \sim \frac{F(L)}{\sigma_{\min}(H)}\sqrt{\mu/N}$；

- 想让 $\varepsilon$ 固定 ⇒
  $$
  N \gtrsim \frac{\mu F(L)^2}{\sigma_{\min}(H)^2\,\varepsilon^2}.
  $$

在本实验里：

- rank-1：$\sigma_{\min} = \sigma_1^{(1)}$ 常数级 ⇒ $N_\varepsilon^{(1)}\sim \mu F(L)^2/\varepsilon^2$；

- rank-3：$\sigma_{\min} = \sigma_3^{(3)}(L)$ 随 L 指数小 ⇒
  $$
  N_\varepsilon^{(3)}\ \sim\ \frac{\mu F(L)^2}{\sigma_3^{(3)}(L)^2\,\varepsilon^2},
  $$
  为 rank-1 所需样本数的一个**指数级放大**。

在实验中，你可以检验如下关系的定性趋势：
$$
\frac{N_\varepsilon^{(3)}}{N_\varepsilon^{(1)}}
\ \approx\
\tilde C\cdot\frac{1}{\sigma_3^{(3)}(L)^2},
$$
即：

- 在相同 L 和 ε 下，rank-3 模型所需样本数随着 $\sigma_3(H_p^{(L)})$ 的衰减而急剧增长；
- 如果你在不同 L 上都做这个对比，会看到 $N_\varepsilon^{(3)}$ 随 L 的增长非常陡，而 rank-1 的 $N_\varepsilon^{(1)}$ 则相对温和。

### 5. 可视化 & 论文里的描述建议

可以给出两类核心图：

1. **条件数图（对应 Part A）**
   - 图 4(a)：$\sigma_1,\sigma_2,\sigma_3$ vs L（rank-3 模型），看出 σ₁、σ₂ 稳定，σ₃ 快速掉头；
   - 图 4(b)：$\log\sigma_3(H_p^{(L)})$ vs L，线性拟合，斜率负，说明指数衰减。
2. **学习误差 vs 样本数（对应 Part B）**
   - 图 4(c)：在固定 L 和 ε 下，对 rank-1 & rank-3，画 $\overline E(N)$ vs N 的双对数图，两条曲线都 ~ 1/√N，但 rank-3 一条整体抬得很高；
   - 图 4(d)：固定目标误差 ε（比如 1e-2），对不同 L 画出 $N_\varepsilon^{(1)}$ 与 $N_\varepsilon^{(3)}$ 的柱状图 / 折线图，直观展示 rank-3 模型的样本复杂度爆炸。

在正文中可以这样总结：

> 我们在第 7 章构造的对角 MPS 例子上进行了数值实验证明。对于 rank-1 模型（7.2A），概率 Hankel 矩阵在所有长度 L 上均为精确秩-1，其唯一非零奇异值保持在常数级；相比之下，在 rank-3 模型（7.2B）中，我们观察到最小奇异值 $\sigma_3(H_p^{(L)})$ 随 L 近似呈指数衰减，而前两个奇异值保持稳定，从而条件数 $\kappa_L=\sigma_1/\sigma_3$ 快速增大。
>
> 在这两个模型上使用相同的 Hankel 谱学习算法，我们评估了点态误差随样本数 N 的收敛行为。尽管两者的误差都大致遵循 $O(N^{-1/2})$ 的经典速率，但要达到相同的目标精度 $\varepsilon$，rank-3 模型所需的样本数远大于 rank-1 模型，其增长趋势与理论预测的 $N\gtrsim \mu/\sigma_{\min}(H_p)^2$ 相一致。这表明，即使 Hankel 秩相同，劣条件（极小的 $\sigma_{\min}$）也会显著放大谱学习的样本复杂度，验证了第 7.3 节中关于条件数依赖的讨论。