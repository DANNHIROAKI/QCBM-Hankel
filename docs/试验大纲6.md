## 实验 6：多视角 Hankel 的好处

（对应第 8.3 章）

### 0. 理论目标回顾

理论里关键是：

- 每个视角 $k$ 有 Hankel 分解：
  $$
  H^{(k)} = O^{(k)} C^{(k)}
  $$

- 联合视角：
  $$
  \mathcal O_{\rm joint}=
  \begin{bmatrix}
  O^{(1)}\\\vdots\\O^{(K)}
  \end{bmatrix},
  \quad
  \mathcal C_{\rm joint}=
  \begin{bmatrix}
  C^{(1)}&\cdots&C^{(K)}
  \end{bmatrix},
  \quad
  H_{\rm joint}=\mathcal O_{\rm joint}\mathcal C_{\rm joint}.
  $$

- 定理 8.7 给出：
  $$
  \sigma_r(H_{\rm joint})
  \ \ge\
  \sqrt{
    \Big(\sum_k \lambda_{\min}(O^{(k)\top}O^{(k)})\Big)
    \Big(\sum_k \lambda_{\min}(C^{(k)}C^{(k)\top})\Big)
  }
  $$
  ⇒ 加视角可以**提高最小奇异值 → 提高条件数**。

**实验目标：**

1. 在真模型上直接验证：
    多视角联合 Hankel 的 $\sigma_r(H_{\rm joint})$ 明显大于单视角 $\sigma_r(H^{(k)})$。
2. 在有限样本谱学习中，对比：
   - 单视角白化（只用一个 $H^{(k)}$）；
   - 多视角联合白化（用所有视角构造的大 Hankel）。
      看：在相同样本数下，多视角是否确实误差更小、方差更小、收敛更快。

------

## 共用设定：模型 + 视角

### 1. ground truth 模型

选一个“中等条件数”的模型，最好是之前实验中已经看到单视角 $\sigma_r$ 不算大的那种：

- 字母表：$\Sigma=\{0,1\}$；

- 键维：$D\in\{3,4\}$；

- 构造方式（和实验 1 一致）：

  1. 对每个位置 $t$ 和每个 $\sigma$ 生成随机复矩阵 $A_t(\sigma)\in\mathbb C^{D\times D}$；

  2. 做 left-canonical 规范化：
     $$
     M_t = \sum_{\sigma}A_t(\sigma)A_t(\sigma)^\dagger,\quad
     A_t(\sigma) \leftarrow M_t^{-1/2}A_t(\sigma);
     $$

  3. 随机归一化边界向量 $\alpha,\beta$。

你也可以直接用第 4 或第 5 个实验里已经保存好的一个「有点劣条件」的 MPS。

### 2. 多视角的定义

为了简单起见，可以**固定长度 L，改切分点**来构造多视角：

- 固定某个 $L$（例如 $L=10$ 或 12）；

- 定义多个切分点：
  $$
  t_{*,1}<t_{*,2}<\cdots<t_{*,K},\quad
  t_{*,k}\in\{2,3,\dots,L-2\}
  $$
  例如：
  $$
  t_{*,1}=3,\quad t_{*,2}=5,\quad t_{*,3}=7.
  $$

**第 k 个视角：**

- 前缀集合：
  $$
  P^{(k)} = \Sigma^{t_{*,k}},
  $$

- 后缀集合：
  $$
  S^{(k)} = \Sigma^{L-t_{*,k}}.
  $$

每个视角都有自己的 Hankel：
$$
H^{(k)}(u,v) = p(uv),\quad u\in P^{(k)},\ v\in S^{(k)}.
$$

> 备注：
>  也可以做「多长度视角」（$L_k=L,L+1,L+2$），构造桥接 Hankel，那就更贴近 13 章的「bridge」版本。实验上先把“同长多切分”玩明白就很够用了。

------

## 实验 6A：真 Hankel 的谱性质（单视角 vs 联合）

### 6A.1 构造真 Hankel 与分解

对固定 $L$ 和选定的几组切分点 $\{t_{*,k}\}_{k=1}^K$：

1. 对每个视角 $k$：

   - 枚举 $P^{(k)}\times S^{(k)}$，对每对 $(u,v)$：

     - 拼接 $x=uv$，用 MPS 计算 $p(x)$；

     - 定义：
       $$
       H^{(k)}(u,v) := p(uv).
       $$

   - 对 $H^{(k)}$ 做 SVD：$H^{(k)}=U^{(k)}\Sigma^{(k)}V^{(k)\top}$，
      得到数值秩 $r$ 和最小非零奇异值：
     $$
     \gamma^{(k)} := \sigma_r(H^{(k)}).
     $$

2. 为了更严格对齐理论中的 $O^{(k)},C^{(k)}$，你可以用**真 WFA 嵌入**构造：

   - 定义概率 Hankel 里的前缀嵌入：
     $$
     \ell_p^{(k)}(u) = \ell(u)\otimes\overline{\ell(u)}\in\mathbb C^{D^2},
     $$

   - 后缀嵌入：
     $$
     r_p^{(k)}(v) = r(v)\otimes\overline{r(v)}\in\mathbb C^{D^2},
     $$

   - 则：
     $$
     H^{(k)}(u,v) = \ell_p^{(k)}(u)^\top r_p^{(k)}(v),
     $$

   - 写成矩阵形式：
     $$
     O^{(k)}\in\mathbb C^{|P^{(k)}|\times r},\quad C^{(k)}\in\mathbb C^{r\times |S^{(k)}|},
     $$
     其中 r ≤ D²，为最小 Hankel 秩（可参考 4.1）。

   这是**理论上的“真分解”**，用于构造联合 Gram（$\lambda_{\min}$）和 $H_{\rm joint}$。

### 6A.2 构造联合视角 Hankel

按 8.3 章的定义：

- 联合可观测矩阵：
  $$
  \mathcal O_{\rm joint}=
  \begin{bmatrix}
  O^{(1)}\\\vdots\\O^{(K)}
  \end{bmatrix}
  \in\mathbb C^{(\sum_k |P^{(k)}|)\times r},
  $$

- 联合可控制矩阵：
  $$
  \mathcal C_{\rm joint}=
  \begin{bmatrix}
  C^{(1)}&\cdots&C^{(K)}
  \end{bmatrix}
  \in\mathbb C^{r\times(\sum_k |S^{(k)}|)}.
  $$

- 联合 Hankel：
  $$
  H_{\rm joint} = \mathcal O_{\rm joint}\mathcal C_{\rm joint}.
  $$

这一步你可以直接用真嵌入 $\ell_p, r_p$ 构造，无需采样。

然后对 $H_{\rm joint}$ 做 SVD：
$$
H_{\rm joint}=U_{\rm joint}\Sigma_{\rm joint}V_{\rm joint}^\top,
\quad
\gamma_{\rm joint}:=\sigma_r(H_{\rm joint}).
$$

### 6A.3 指标与预期

**1）最小奇异值对比**

- 收集：
  $$
  \{\gamma^{(1)},\dots,\gamma^{(K)},\gamma_{\rm joint}\}.
  $$

- 画一张柱状图或表格，比较每个视角单独的 $\sigma_r(H^{(k)})$ 与联合 $\sigma_r(H_{\rm joint})$。

预期：

- $\gamma_{\rm joint}$ 显著大于每个 $\gamma^{(k)}$，验证联合 Gram 累积提升 $\lambda_{\min}$ 的效果。

**2）Gram 下界验证（可选）**

- 对每个视角，计算：
  $$
  G_O^{(k)}:=O^{(k)\top}O^{(k)},\quad
  G_C^{(k)}:=C^{(k)}C^{(k)\top}.
  $$

- 计算：
  $$
  \Lambda_O := \sum_k \lambda_{\min}(G_O^{(k)}),
  \quad
  \Lambda_C := \sum_k \lambda_{\min}(G_C^{(k)}).
  $$

- 比较：
  $$
  \gamma_{\rm joint}\quad\text{vs}\quad
  \sqrt{\Lambda_O\Lambda_C}.
  $$

这里主要是 sanity check：$\gamma_{\rm joint}$ 应不小于这个下界（数值误差内），符合定理 8.7 的方向性。

------

## 实验 6B：多视角联合白化对谱学习稳定性的影响

这里我们要做的是：用同一个样本集，在**单视角白化** vs **多视角联合白化** 下跑谱学习，然后比较：

- 点态误差的大小；
- 学习的方差（不同重复实验之间的波动）。

### 6B.1 设定

- 使用与 6A 相同的 ground truth 模型和视角 $\{t_{*,k}\}$。
- 固定长度 L（如 10 或 12）。
- 固定样本数 $N$（比如 $N=2\times 10^4$ 或 $5\times 10^4$），你也可以扫描几组 N。
- 对每个设置重复 R 次（例如 R=20），用于估计统计波动。

### 6B.2 单视角谱学习（baseline）

对每个视角 $k$，我们可以独立执行一次谱学习。为简单起见，可以重点挑一个“代表视角”（比如中间切分 $t_{*,2}$）来作为 baseline；或者同时展示多个视角的误差分布。

**单视角流程（固定 k）：**

1. 用样本构造该视角的经验 Hankel $\widehat H^{(k)}$：

   - 切分所有样本 $x^{(i)}$ 为 $u^{(i)},v^{(i)}$ 根据 $t_{*,k}$；
   - 只对 $(u^{(i)},v^{(i)})\in P^{(k)}\times S^{(k)}$ 的条目计数；
   - 归一化得到 $\widehat H^{(k)}$。

2. 对 $\widehat H^{(k)}$ 做秩‑r SVD，构造单视角白化矩阵：
   $$
   \widehat W_L^{(k)},\widehat W_R^{(k)}.
   $$

3. 构造单视角嵌入 $\phi^{(k)}(u),\psi^{(k)}(v)$，以及单视角估计 $\widehat p^{(k)}(x)$，方法与实验 5 的“单长度”方案一样：
   $$
   \widehat H^{(k)} \approx U_r^{(k)}\Sigma_r^{(k)}V_r^{(k)\top},\quad
   \phi^{(k)}(u)=\Sigma_r^{(k)1/2}U_r^{(k)\top}e_u,\quad
   \psi^{(k)}(v)=\Sigma_r^{(k)1/2}V_r^{(k)\top}e_v,
   $$

   $$
   \widehat p^{(k)}(uv) := \phi^{(k)}(u)^\top\psi^{(k)}(v).
   $$

4. 点态误差度量：

   - 枚举或随机采样一批 test 序列 $x$；

   - 定义单视角误差：
     $$
     E_{\text{single},k} := \max_{x\in\text{test}}|\widehat p^{(k)}(x)-p(x)|.
     $$

对 R 次重复取平均/中位数 $\overline E_{\text{single},k}$，以及标准差。

### 6B.3 多视角联合白化

**核心 idea：** 把所有视角的 Hankel 信息统一到一个大矩阵里，做一次 rank‑r SVD 来白化，相当于用联合 Gram $\mathcal O_{\rm joint},\mathcal C_{\rm joint}$ 来确定坐标。

#### (1) 构造联合经验 Hankel

方式一（直接 union）：

- 构造全局前缀集合：
  $$
  P_{\rm all} := \bigcup_{k=1}^K P^{(k)},
  $$

- 全局后缀集合：
  $$
  S_{\rm all} := \bigcup_{k=1}^K S^{(k)}.
  $$

定义统一索引：给 $P_{\rm all},S_{\rm all}$ 每个元素编号。

每个样本 $x^{(i)}$ 在不同视角下产生 K 对 (u,v)：

- 第 k 个视角：
  $$
  x^{(i)} = u_k^{(i)} v_k^{(i)},\quad
  u_k^{(i)}\in P^{(k)},\ v_k^{(i)}\in S^{(k)}\subset P_{\rm all}\times S_{\rm all}.
  $$

我们用所有这些 pairs 来更新一个**大 Hankel**：
$$
\widehat H_{\rm all}(u,v) =
\frac{1}{NK} \sum_{i=1}^N \sum_{k=1}^K
\mathbf 1\{u_k^{(i)} = u,\ v_k^{(i)}=v\}.
$$

- 这样每个样本提供 K 个观测；
- 分母 $NK$ 确保 $\widehat H_{\rm all}$ 仍然是某种“联合频率”的估计；
- 真值对应的 $H_{\rm all}$ 就是用 $P_{\rm all},S_{\rm all}$ 构造的大 Hankel，包含所有视角（正好是理论里的 $H_{\rm joint}$ 的“样本版”）。

#### (2) 对 $\widehat H_{\rm all}$ 做联合 SVD

- SVD：
  $$
  \widehat H_{\rm all} = U_{\rm all}\Sigma_{\rm all}V_{\rm all}^\top;
  $$

- 取 rank‑r 截断：
  $$
  U_r^{\rm all}, \Sigma_r^{\rm all}, V_r^{\rm all};
  $$

- 联合白化矩阵：
  $$
  W_L^{\rm joint} = \Sigma_r^{\rm all\,-1/2} U_r^{\rm all\top},\quad
  W_R^{\rm joint} = V_r^{\rm all}\Sigma_r^{\rm all\,-1/2}.
  $$

此时 $\Sigma_r^{\rm all}$ 的最小奇异值就对应数值版的 $\gamma_{\rm joint}$。

#### (3) 多视角嵌入与预测

有了联合 U,V 之后，我们可以定义所有前缀、后缀的联合嵌入：

- 对任意 $u\in P_{\rm all}$：
  $$
  \phi_{\rm joint}(u)=\Sigma_r^{\rm all\,1/2}U_r^{\rm all\top}e_u;
  $$

- 对任意 $v\in S_{\rm all}$：
  $$
  \psi_{\rm joint}(v)=\Sigma_r^{\rm all\,1/2}V_r^{\rm all\top}e_v.
  $$

对于某个具体的测试串 $x$，因为我们有多个切分点 $t_{*,k}$，可以从不同视角得到多个 (u,v) 表达：

- 第 k 视角：$x=u_k v_k$。

因此我们有多种预测方式：

1. **单视角读取（用联合白化）：**
   $$
   \widehat p_{\rm multi}^{(k)}(x) := \phi_{\rm joint}(u_k)^\top \psi_{\rm joint}(v_k).
   $$

2. **多视角平均：**
   $$
   \widehat p_{\rm multi}(x) = \frac1K\sum_{k=1}^K \widehat p_{\rm multi}^{(k)}(x).
   $$

实践上，用平均会更稳定一些，可以定义最终 multi-view 估计为 $\widehat p_{\rm multi}(x)$。

同样地，对负值截断 + 归一化，确保 $\widehat p_{\rm multi}$ 是一个合法分布。

#### (4) 误差与方差

在每次重复实验中：

- 定义单视角 baseline（例如选视角 $k_*$ 为中间切分）：
  $$
  E_{\text{single}} := \max_{x\in\text{test}}|\widehat p^{(k_*)}(x)-p(x)|.
  $$

- 多视角误差：
  $$
  E_{\text{multi}} := \max_{x\in\text{test}}|\widehat p_{\rm multi}(x)-p(x)|.
  $$

对 R 次重复，计算：

- 均值或中位数：
  $$
  \overline E_{\text{single}},\quad \overline E_{\text{multi}},
  $$

- 标准差（衡量稳定性）：
  $$
  \mathrm{Std}[E_{\text{single}}],\quad \mathrm{Std}[E_{\text{multi}}].
  $$

------

## 6B.4 指标与预期图像

**1）谱性质 vs 误差的对应**

先用真 Hankel 对比：

- 单视角 $\gamma^{(k)} = \sigma_r(H^{(k)})$ vs 联合 $\gamma_{\rm joint}=\sigma_r(H_{\rm joint})$，一般可看到：
  $$
  \gamma_{\rm joint} \gg \min_k \gamma^{(k)}.
  $$

再看学习误差：

- 同样 N 下，多视角误差：
  $$
  \overline E_{\text{multi}} \ll \overline E_{\text{single}},
  $$
  且方差 $\mathrm{Std}[E_{\text{multi}}]$ 更小。

- 把误差乘上 $\gamma$ 后再比，会发现：
  $$
  \overline E_{\text{single}} \cdot \gamma^{(k_*)}
  \quad\text{和}\quad
  \overline E_{\text{multi}}\cdot \gamma_{\rm joint}
  $$
  尺度更加接近，符合「误差放大约 $\propto 1/\gamma$」的理论（11.1/11.4）。

**2）可视化方案**

- 图 6(a)：柱状图/折线图
  - x 轴：视角 k 及 “joint”；
  - y 轴：$\sigma_r(H^{(k)})$ 和 $\sigma_r(H_{\rm joint})$。
     清楚展示联合时最小奇异值抬高。
- 图 6(b)：误差对比
  - x 轴：不同设置（single vs multi）；
  - y 轴：$\overline E$（可以有误差棒表示标准差）。
     多视角柱子显著低且误差棒短，体现「更稳」。

如果你愿意再精一点，还可以画：

- $\overline E\cdot\gamma$ 的对比，看是否更接近某个共同常数（印证端到端误差界中的 $1/\gamma$ 依赖）。

------

## 论文式总结句示例

可以在 8.3 章或实验部分这样写：

> We construct multiple Hankel “views” of the same underlying QCBM/MPS model by varying the cut position $t_\star$ for a fixed sequence length $L$. Each view yields a Hankel matrix $H^{(k)}$ with rank $r$, and we build a joint Hankel representation $H_{\rm joint}$ by stacking the corresponding observability and controllability embeddings as in Section 8.3. Numerically, we observe that the smallest non-zero singular value $\sigma_r(H_{\rm joint})$ is significantly larger than that of any individual view, in line with Theorem 8.7 and the Gram-based lower bounds.
>
> We then compare spectral learning with single-view whitening versus joint multi-view whitening. For a fixed sample size $N$, multi-view whitening corresponds to performing a rank-$r$ SVD on a large Hankel matrix constructed from the union of all prefix–suffix sets, and reusing the resulting embeddings across all views. In this setting, the multi-view learner consistently achieves smaller pointwise reconstruction error and reduced variance across repeated trials, especially in the small-sample regime. After normalizing by the respective smallest singular values, the error profiles for single- and multi-view learners become comparable, confirming that the robustness improvement is primarily due to the increase of $\sigma_r(H)$ induced by combining multiple Hankel views.